---
title: "Why Bayes?"
author: "Stefano Coretta"
institute: "University of Edinburgh"
date: "2022/02/22"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css:
      - default
      - ipa-fonts.css
      - custom.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
knitr::opts_knit$set(root.dir = here::here())
options(htmltools.dir.version = FALSE)
library(tidyverse)
theme_set(theme_minimal())
library(xaringanExtra)
use_xaringan_extra(c("panelset", "tachyons"))
```

# The big questions

<br>
<br>

.f2[Is there (not) an effect of X on Y?]

--

<br>

.f2[What is the effect of X on Y?]

--

<br>

.f2[Given two or more hypotheses, which is the most compatible with the data?]

---

# Statistical inference: two approaches to probability

--

.pull-left[
## .orange[Frequentist]

$$
\begin{aligned}
      & x_d = x_1 - x_2 \\
H_0:  \quad & x_d = 0 \\
H_1:  \quad & x_d \neq 0 \\
\end{aligned}
$$

<br>

**Reject $H_0$.**

*Caveat*: $H_1$ can be any of:
* $x_d > 0$,
* $x_d < 0$,
* $\{x_d \in \mathbb{R} | x_d \neq 0\}$, i.e. "any real number except 0".
]

--

.pull-right[
## .green[Bayesian]

$$
\begin{aligned}
P(\theta|d) & = P(d|\theta) \times P(\theta) \\
            & = \text{likelihood} \times \text{prior belief}
\end{aligned}
$$
]

---

# Statistical inference: two approaches to probability

.pull-left[
## .orange[Frequentist]

<br>
<br>
<br>
<br>

$$\huge P(d|H_0)$$

]

.pull-right[
## .green[Bayesian]

<br>
<br>
<br>
<br>

$$\huge P(H|d)$$

]

---

# Statistical inference: two approaches to probability

.pull-left[
## .orange[Frequentist]

<br>
<br>
<br>
<br>

$$\huge P(d|H_0)$$

<br>

What is the probability of the data $d$ assuming that the hypothesis $H_0$ is true?

]

.pull-right[
## .green[Bayesian]

<br>
<br>
<br>
<br>

$$\huge P(H|d)$$

<br>

What is the probability of the hypothesis $H$ given the data $d$.

]

---

class: right middle inverse

# Reason 1

<br>
<br>

## .orange[LMERs] only provide evidence for rejecting the Null Hypothesis

<br>

## while .green[BRMs] can compare any hypothesis (not just null vs alternative)








---

# Intuitiveness

.bg-washed-yellow.b--orange.ba.bw2.br3.shadow-5.ph4.mt5[

The ***p*-value** is the probability of finding a certain difference or a bigger difference, assuming that there is no difference. ðŸ˜±ðŸ˜±ðŸ˜±

]

--

.center[
<div style="width:400px;height:400px;position:relative;"><iframe src="https://giphy.com/embed/xkQztEUp95DS8" width="100%" height="100%" style="position:absolute" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></div>
]

---

# Intuitiveness

.bg-washed-green.b--dark-green.ba.bw2.br2.shadow-5.ph2.mt3[

A **Bayesian posterior probability distribution** tells you the probability that the estimated effect falls within a specific range of values.

]

```{r posterior, echo=FALSE, include=TRUE, fig.width=7, fig.height=4, out.width="800px", fig.retina=2, fig.align="center"}
x <- seq(-50, 200, by = 0.01)
y = dnorm(x, 75, 30)
ggplot() +
  aes(x, y) +
  geom_ribbon(aes(x = x, ymin = 0, ymax = y), alpha = 0.3, fill = "#8970FF") +
  geom_ribbon(aes(x = ifelse(x >= 15 & x <= 135, x, NA), ymin = 0, ymax = y), alpha = 0.4, fill = "#8970FF") +
  geom_ribbon(aes(x = ifelse(x >= 45 & x <= 105, x, NA), ymin = 0, ymax = y), alpha = 0.7, fill = "#8970FF") +
  geom_line(size = 2, colour = "#FFA70B") +
  annotate(
    "segment",
    x = 75-30, xend = 75+30, y = 0.015, yend = 0.015,
    arrow = arrow(ends = "both", angle = 90, length = unit(.2, "cm")),
    size = 1
  ) +
  annotate("label", x = 75, y = 0.015, label = "68%") +
  annotate(
    "segment",
    x = 75-60, xend = 75+60, y = 0.0175, yend = 0.0175,
    arrow = arrow(ends = "both", angle = 90, length = unit(.2, "cm")),
    size = 1
  ) +
  annotate("label", x = 75, y = 0.0175, label = "95%") +
  annotate(
    "segment",
    x = 75-90, xend = 75+90, y = 0.02, yend = 0.02,
    arrow = arrow(ends = "both", angle = 90, length = unit(.2, "cm")),
    size = 1
  ) +
  annotate("label", x = 75, y = 0.02, label = "99.7 â‰ˆ 100%") +
  # scale_x_continuous(breaks = c(-4:4), labels = c("", expression(paste(-3, sigma)), expression(paste(-2, sigma)), expression(paste(-1, sigma)), expression(paste(mu)), expression(paste(+1, sigma)), expression(paste(+2, sigma)), expression(paste(+3, sigma)), "")) +
  labs(
    x = element_blank(), y = element_blank()
  )
```

---

# A frequentist CI is not what most people think it is

<iframe id="inlineFrameExample"
    title="Inline Frame Example"
    width="1000"
    height="450"
    src="https://rpsychologist.com/d3/ci/">
</iframe>


---

# LMER is based on an imaginary set of experiments

.center[![](img/imaginary.jpg)]

---

class: right middle inverse

# Reason 2

<br>
<br>

## .green[Bayesian] inference is more intuitive than .orange[frequentist] inference








---

# Fitting issues

.center[
![](img/convergence.jpg)
]

---

class: right middle inverse

# Reason 3

<br>
<br>

## .green[BRMs] always converge, .orange[LMERs] don't always do








---

# Type-I error rates and co

.bg-washed-yellow.b--orange.ba.bw2.br3.shadow-5.ph4.mt5[
**Heisig and Schaeffer 2019**

- Not including random slopes increases the false-positive rate (Type I error rate).
]

.bg-washed-yellow.b--orange.ba.bw2.br3.shadow-5.ph4.mt5[

**Lindley's paradox**

- Frequentist model rejects $H_0$ while Bayesian model favours it.
]

---

class: right middle inverse

# Reason 4

<br>
<br>

## .orange[LMERs] are more sensitive to error and can lead to anti-conservative *p*-values









---

# Memory

.center[
![:scale 50%](./img/goldfish.png)
]

--

.f6[
(Actually, it's a myth: https://thinking.umwblogs.org/2020/02/26/goldfish-memory/)
]

---

# Memory

.center[
![:scale 70%](./img/prior-update.png)
]


---

class: right middle inverse

# Reason 5

<br>
<br>

## You can embed prior knowledge in .green[BRMs] while you can't in .orange[LMERs]









---

# Time management

.center[
![](./img/timeline.png)
]

---

# Time management

.center[
![](./img/timeline-full.png)
]

---

class: right middle inverse

# Reason 6

<br>
<br>

## .orange[LMERs] require as much work as .green[BRMs]







---

# *In Baiulo veritas*

<iframe src="https://seeing-theory.brown.edu/bayesian-inference/index.html#section3" style="border:none;" width="100%" height="100%">


---

class: right middle inverse

# Reason 7

<br>
<br>

## .green[BRMs] converge towards the true value in the long run, while .orange[LMERs] do not




---

# Summary

* Reason 1: .orange[LMERs] only provide evidence for rejecting the Null Hypothesis, while .green[BRMs] can compare any hypothesis (not just null vs alternative)

* Reason 2: .green[Bayesian] inference is more intuitive than .orange[frequentist] inference

* Reason 3: .green[BRMs] always converge, .orange[LMERs] don't always do

* Reason 4: .orange[LMERs] are more sensitive to error and can lead to anti-conservative *p*-values

* Reason 5: You can embed prior knowledge in .green[BRMs] while you can't in .orange[LMERs]

* Reason 6: .orange[LMERs] require as much work as .green[BRMs]

* Reason 7: .green[BRMs] converge towards the true value in the long run, while .orange[LMERs] do not


---

# LMERs can be useful

- In corporate decisions, where a categorical yes/no outcome is necessary.

- When you are only interested in rejecting the Null Hypothesis (with the caveat that you must run a prospective power analysis and reduce researcher's degrees of freedom to a minimum).

- You can use equivalence testing with LMERs if you are interested in accepting the Null Hypothesis (same caveat as above).


---

class: center middle inverse

# Questions?
